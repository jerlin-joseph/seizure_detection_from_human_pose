{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26542ba-7102-4052-92e1-3f662ab1176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc852533-06b9-4428-af61-db7fb02b48d9",
   "metadata": {},
   "source": [
    "### Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7a4d5-361a-477f-8713-c47a4d5bca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "\n",
    "# Check MMPose installation\n",
    "import mmpose\n",
    "print('mmpose version:', mmpose.__version__)\n",
    "\n",
    "# Check MMPose installation\n",
    "import mmcv\n",
    "print('mmcv version:', mmcv.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print('cuda version:', get_compiling_cuda_version())\n",
    "print('compiler information:', get_compiler_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0851d6-14a0-48b0-b10a-a8d839440a76",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a50ed1-8419-4e88-832b-d5b80a29f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import logging\n",
    "import mimetypes\n",
    "import os\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "import cv2\n",
    "import json_tricks as json\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "from mmengine.logging import print_log\n",
    "\n",
    "from mmpose.apis import (_track_by_iou, _track_by_oks,\n",
    "                         convert_keypoint_definition, extract_pose_sequence,\n",
    "                         inference_pose_lifter_model, inference_topdown,\n",
    "                         init_model)\n",
    "from mmpose.models.pose_estimators import PoseLifter\n",
    "from mmpose.models.pose_estimators.topdown import TopdownPoseEstimator\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.visualization import FastVisualizer\n",
    "from mmpose.structures import (PoseDataSample, merge_data_samples,\n",
    "                               split_instances)\n",
    "from mmpose.utils import adapt_mmdet_pipeline\n",
    "from mmengine import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99dcc05-9e1b-4b08-bdfe-d4b44aafe8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging settings\n",
    "logging.basicConfig(\n",
    "    filename='data/pose/execution_times.log',  # Log file name\n",
    "    filemode='a',                   # Append to the file ('w' for overwrite)\n",
    "    level=logging.INFO,             # Log level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Log format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4adaf4-ce67-443f-97dd-782b759e09d1",
   "metadata": {},
   "source": [
    "### Initialize pose, detection and lifter models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1445926-2d42-4170-9486-f6435186365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from mmdet.apis import inference_detector, init_detector\n",
    "    has_mmdet = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    has_mmdet = False\n",
    "\n",
    "# Configuration paths from 17 keypoints\n",
    "det_config = 'mmpose/demo/mmdetection_cfg/rtmdet_m_640-8xb32_coco-person.py'\n",
    "det_checkpoint = 'https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth'\n",
    "pose_estimator_config = 'mmpose/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb256-420e_body8-256x192.py'\n",
    "pose_estimator_checkpoint = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.pth'\n",
    "pose_lifter_config = 'mmpose/configs/body_3d_keypoint/video_pose_lift/h36m/video-pose-lift_tcn-243frm-supv-cpn-ft_8xb128-200e_h36m.py'\n",
    "pose_lifter_checkpoint = 'https://download.openmmlab.com/mmpose/body3d/videopose/videopose_h36m_243frames_fullconv_supervised_cpn_ft-88f5abbb_20210527.pth'\n",
    "\n",
    "# Configuration paths from 133 keypoints\n",
    "det_wholebody_config = 'mmpose/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
    "det_wholebody_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'   \n",
    "pose_estimator_wholebody_config = 'mmpose/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py'\n",
    "pose_estimator_wholebody_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_wholebody_384x288_dark-f5726563_20200918.pth'\n",
    " \n",
    "     \n",
    "# Define parameters\n",
    "show_visualization = True\n",
    "save_predictions = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Initialize models\n",
    "detector = init_detector(det_config, det_checkpoint, device=device)\n",
    "detector.cfg = adapt_mmdet_pipeline(detector.cfg)\n",
    "\n",
    "pose_estimator = init_model( pose_estimator_config,\n",
    "                             pose_estimator_checkpoint,\n",
    "                             device=device )\n",
    "\n",
    "pose_estimator_wholebody = init_model( pose_estimator_wholebody_config,\n",
    "                             pose_estimator_wholebody_checkpoint,\n",
    "                             device=device )\n",
    "\n",
    "assert isinstance(pose_estimator, TopdownPoseEstimator), 'Only \"TopDown\" model is supported for 2D pose detection'\n",
    "assert isinstance(pose_estimator_wholebody, TopdownPoseEstimator), 'Only \"TopDown\" model is supported for 2D pose detection'\n",
    "\n",
    "# Define visualizer settings\n",
    "det_kpt_color = pose_estimator.dataset_meta.get('keypoint_colors', None)\n",
    "det_dataset_skeleton = pose_estimator.dataset_meta.get('skeleton_links', None)\n",
    "det_dataset_link_color = pose_estimator.dataset_meta.get('skeleton_link_colors', None)\n",
    "\n",
    "pose_lifter = init_model(pose_lifter_config,\n",
    "                         pose_lifter_checkpoint,\n",
    "                         device=device )\n",
    "\n",
    "assert isinstance(pose_lifter, PoseLifter), 'Only \"PoseLifter\" model is supported for 2D-to-3D lifting'\n",
    "\n",
    "'''# Set up the visualizer. Works for the 3D\n",
    "pose_lifter.cfg.visualizer.radius = 3\n",
    "pose_lifter.cfg.visualizer.line_width = 1\n",
    "pose_lifter.cfg.visualizer.det_kpt_color = det_kpt_color\n",
    "pose_lifter.cfg.visualizer.det_dataset_skeleton = det_dataset_skeleton\n",
    "pose_lifter.cfg.visualizer.det_dataset_link_color = det_dataset_link_color\n",
    "visualizer = VISUALIZERS.build(pose_lifter.cfg.visualizer)\n",
    "visualizer.set_dataset_meta(pose_lifter.dataset_meta) '''\n",
    "# Init visualizer\n",
    "pose_estimator_wholebody.cfg.visualizer.radius = 3\n",
    "pose_estimator_wholebody.cfg.visualizer.line_width = 1\n",
    "visualizer = VISUALIZERS.build(pose_estimator_wholebody.cfg.visualizer)\n",
    "# The dataset_meta is loaded from the checkpoint and then passed to the model in init_pose_estimator\n",
    "visualizer.set_dataset_meta(pose_estimator_wholebody.dataset_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201de66-973e-4fdd-8554-e4ec28b882ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#det_model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf10bc3-77bc-48a7-a381-850d32d5b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''det_model.cuda()\n",
    "pose_model.cuda()\n",
    "lifter_model.cuda()\n",
    "None'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca1884-553a-4686-adcb-272fdcd2fe56",
   "metadata": {},
   "source": [
    "#### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc055a2-7227-4157-a471-65c7f52db379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bboxes():\n",
    "    ''' retrieve the bboxes that were added manually to the video'''\n",
    "    bboxes = pd.read_csv('data/bboxes.csv')\n",
    "    return bboxes\n",
    "\n",
    "def get_video_reader(vidfile):\n",
    "    \"\"\"Initialize video reader and check fps.\"\"\"\n",
    "    vid_reader = mmcv.VideoReader(vidfile)\n",
    "    if vid_reader.fps < 29 or vid_reader.fps > 31:\n",
    "        print('fps = ' + str(vid_reader.fps))\n",
    "    return vid_reader\n",
    "\n",
    "def get_init_bbox(vidfile_name, bboxes):\n",
    "    \"\"\"Get the initial bounding box from the bounding boxes dataframe.\"\"\"\n",
    "    init_bbox = bboxes[bboxes['fname'] == vidfile_name].loc[:, 'xmin':'ymax']\n",
    "    \n",
    "    if init_bbox['xmin'].item() > init_bbox['xmax'].item():\n",
    "        init_bbox['xmin'], init_bbox['xmax'] = init_bbox['xmax'], init_bbox['xmin']\n",
    "    if init_bbox['ymin'].item() > init_bbox['ymax'].item():\n",
    "        init_bbox['ymin'], init_bbox['ymax'] = init_bbox['ymax'], init_bbox['ymin']\n",
    "    \n",
    "    return init_bbox\n",
    "\n",
    "def initialize_video_writer(output_path, width, height):\n",
    "    \"\"\"Initialize video writer for saving the output with pose visualization.\"\"\"\n",
    "    #print('Starting initialize_video_writer ...')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    vid_writer = cv2.VideoWriter(output_path, fourcc, 30, (width, height))\n",
    "    return vid_writer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5db1cd-120d-4964-8d08-d0e6360af916",
   "metadata": {},
   "source": [
    "#### Save pose to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b621dbd-2f17-4c8f-8c2d-51675cc88b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_combined_data(result_pose, result_3d, bbox):\n",
    "    \"\"\"\n",
    "    Combines data from result_pose, result_3d, and bbox into a single array.\n",
    "    \n",
    "    Args:\n",
    "        result_pose (numpy.ndarray): Pose data.\n",
    "        result_3d (numpy.ndarray): 3D data with dimensions (n, k, 3).\n",
    "        bbox (array-like): Bounding box in [x_min, y_min, x_max, y_max] format.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Combined data array.\n",
    "    \"\"\"\n",
    "    # Step 1: Flatten result_pose\n",
    "    try:\n",
    "        #print(f\"result_ pose: {result_pose}\")\n",
    "        flattened_pose = result_pose.flatten()\n",
    "        #print(f\"Flattened pose: {flattened_pose}\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error flattening result_pose: {e}\")\n",
    "\n",
    "    # Step 2: Extract Z-coordinates from the first element of result_3d\n",
    "    try:\n",
    "        print(f\"result_3d: {result_3d}\")\n",
    "        z_coords = result_3d[:, 2].tolist()\n",
    "        print(f\"Extracted Z-coordinates: {z_coords}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Z-coordinates from result_3d: {e}\")\n",
    "\n",
    "    # Step 3: Ensure bbox is a numpy array\n",
    "    try:\n",
    "        bbox_array = np.array(bbox)\n",
    "        print(f\"BBox array: {bbox_array}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting bbox to numpy array: {e}\")\n",
    "\n",
    "    # Step 4: Append all components\n",
    "    try:\n",
    "        combined_data = np.append(\n",
    "            np.append(flattened_pose, z_coords),\n",
    "            bbox_array\n",
    "        )\n",
    "        print(f\"Combined data: {combined_data}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining data: {e}\")\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Example usage\n",
    "result_pose = np.random.rand(17, 3)  # Example 17 keypoints with 3 coordinates each\n",
    "result_3d = np.random.rand(1, 17, 3)  # Example 3D data (1 person, 17 keypoints, 3 coordinates)\n",
    "bbox = [140, 44, 450, 450]  # Example bbox in [x_min, y_min, x_max, y_max] format\n",
    "\n",
    "combined_data = create_combined_data(result_pose, result_3d, bbox)\n",
    "print(\"Final combined data:\", combined_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c239b-b9cf-4092-9c3e-a303519e8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pose_results_to_csv(csv_writer, all_keypoints_wholebody, all_keypoints_3d, all_bboxes):\n",
    "    \"\"\"Save all pose and 3D results to the CSV file.\"\"\"\n",
    "\n",
    "    header = []\n",
    "    for kpt in range(133):\n",
    "        header.append(f'x{kpt}')\n",
    "        header.append(f'y{kpt}')\n",
    "        header.append(f'conf{kpt}')\n",
    "    for kpt_3d in range(17):\n",
    "        header.append(f'z{kpt_3d}') # this used to be kpt, which made me add a renaming step in the classifier\n",
    "    header.append('xmin')\n",
    "    header.append('ymin')\n",
    "    header.append('xmax')\n",
    "    header.append('ymax')\n",
    "    header.append('bbox_conf')\n",
    "    # write header\n",
    "    csv_writer.writerow(header)\n",
    "     \n",
    "    # write data\n",
    "    for result_pose, result_3d, bbox in zip(all_keypoints_wholebody, all_keypoints_3d, all_bboxes):\n",
    "        try:\n",
    "            # Create the row by combining data and adding a new value (e.g., new_value)\n",
    "            combined_data = np.append(\n",
    "                            np.append(result_pose.flatten(), result_3d[:, 2].tolist()), \n",
    "                            bbox)\n",
    "            # Add conf score to the end\n",
    "            combined_data = np.append(combined_data, 1)\n",
    "\n",
    "            csv_writer.writerow(combined_data)\n",
    "            \n",
    "            #csv_writer.writerow(np.append(np.append(result_pose.flatten(), result_3d[0, :, 2].tolist()), bbox))\n",
    "        except IndexError:\n",
    "            #print(result_3d)\n",
    "            raise Exception('something about dimensions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d761913-8bad-463c-b46c-95abc3c35f65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772061f2-74fb-4bce-8112-92a674cd4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_frame(frame, data_sample_chosen, vid_writer,extra_bbox=None):\n",
    "    #Visualize the pose results on a frame and save the frame to the video.\n",
    "\n",
    "    data_samples = merge_data_samples([data_sample_chosen])\n",
    "\n",
    "    visualizer.add_datasample(\n",
    "        'result',\n",
    "        frame,\n",
    "        data_sample=data_samples,\n",
    "        draw_gt=False,\n",
    "        draw_heatmap=False,\n",
    "        draw_bbox=True,\n",
    "        show=False,\n",
    "        wait_time=0,\n",
    "        out_file=None,\n",
    "        kpt_thr=0.3)\n",
    "\n",
    "    processed_frame = visualizer.get_image()\n",
    "    \n",
    "    #vis_result_bgr = cv2.cvtColor(processed_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Add an extra bounding box if provided\n",
    "    if extra_bbox is not None:\n",
    "        # Draw the extra bounding box (distinct color, e.g., blue)\n",
    "        # Use .iloc[0] to avoid FutureWarning\n",
    "        xmin = int(extra_bbox['xmin'].iloc[0])\n",
    "        ymin = int(extra_bbox['ymin'].iloc[0])\n",
    "        xmax = int(extra_bbox['xmax'].iloc[0])\n",
    "        ymax = int(extra_bbox['ymax'].iloc[0])\n",
    "        cv2.rectangle(processed_frame, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)  # Blue box with thickness 2\n",
    "    \n",
    "    \n",
    "    #cv2.imwrite('data/pose/videos/output_pose_image.png', vis_result_bgr)\n",
    "    vid_writer.write(processed_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e1eba-ecaf-4573-9dd9-e9598138b32f",
   "metadata": {},
   "source": [
    "#### Find 2d 133 keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f8163-f029-41f7-a8cb-98bf06f5d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe remove this function move the 2d call into pose_estimation()\n",
    "def get_2d_keypoints_from_frame(detector, frame, bbox_init, pose_estimator, visualizer):\n",
    "    \"\"\" Find whole body keypoints of one image. 133 keypoints\n",
    "    \"\"\"\n",
    "    # estimate pose results for current image\n",
    "    #pose_est_results = inference_topdown(pose_estimator, frame, bboxes)\n",
    "    pose_est_results = inference_topdown(pose_estimator, frame, np.array(bbox_init)[None],'xyxy')[0]\n",
    "    \n",
    "    return pose_est_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a8c64-e738-4117-a578-a5e4ea0ac158",
   "metadata": {},
   "source": [
    "#### Find 3d pose lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8bf13-ba6f-486b-a1f9-4440ab1ba4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_keypoints_from_frame(detector, frame, frame_idx, pose_estimator,\n",
    "                      pose_est_results_last, pose_est_results_list, next_id,\n",
    "                      pose_lifter, visualize_frame, visualizer):\n",
    "    \"\"\"Visualize detected and predicted keypoints of one image.\n",
    "    Pipeline -> Detect -> 2d pose estimate -> 3d pose lift\n",
    "    works on 17 keypoints\n",
    "    \"\"\"\n",
    "    \n",
    "    pose_lift_dataset = pose_lifter.cfg.test_dataloader.dataset\n",
    "    pose_lift_dataset_name = pose_lifter.dataset_meta['dataset_name']\n",
    "\n",
    "    # First stage: conduct 2D pose detection in a Topdown manner\n",
    "    # use detector to obtain person bounding boxes\n",
    "    det_result = inference_detector(detector, frame)\n",
    "    pred_instance = det_result.pred_instances.cpu().numpy()\n",
    "\n",
    "    # filter out the person instances with category and bbox threshold\n",
    "    # e.g. 0 for person in COCO\n",
    "    bboxes = pred_instance.bboxes\n",
    "    bboxes = bboxes[np.logical_and(pred_instance.labels == 0,  # Assuming person category is 0\n",
    "                                   pred_instance.scores > 0.3)]  # Example threshold\n",
    "\n",
    "    # estimate pose results for current image\n",
    "    pose_est_results = inference_topdown(pose_estimator, frame, bboxes)\n",
    "    \n",
    "    # Assuming you want to use IOU tracking\n",
    "    _track = partial(_track_by_iou)  # Use IOU for tracking\n",
    "\n",
    "    pose_det_dataset_name = pose_estimator.dataset_meta['dataset_name']\n",
    "    #print(f\"pose_det_dataset_name = {pose_det_dataset_name}\")\n",
    "    pose_est_results_converted = []\n",
    "\n",
    "    # convert 2d pose estimation results into the format for pose-lifting\n",
    "    # such as changing the keypoint order, flipping the keypoint, etc.\n",
    "    for i, data_sample in enumerate(pose_est_results):\n",
    "        pred_instances = data_sample.pred_instances.cpu().numpy()\n",
    "        keypoints = pred_instances.keypoints\n",
    "       \n",
    "        # Trim keypoints to only the first 17\n",
    "        #keypoints = keypoints[:, :17, :] #jerlin\n",
    "        \n",
    "        # calculate area and bbox\n",
    "        if 'bboxes' in pred_instances:\n",
    "            areas = np.array([(bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "                              for bbox in pred_instances.bboxes])\n",
    "            pose_est_results[i].pred_instances.set_field(areas, 'areas')\n",
    "        else:\n",
    "            areas, bboxes = [], []\n",
    "            for keypoint in keypoints:\n",
    "                xmin = np.min(keypoint[:, 0][keypoint[:, 0] > 0], initial=1e10)\n",
    "                xmax = np.max(keypoint[:, 0])\n",
    "                ymin = np.min(keypoint[:, 1][keypoint[:, 1] > 0], initial=1e10)\n",
    "                ymax = np.max(keypoint[:, 1])\n",
    "                areas.append((xmax - xmin) * (ymax - ymin))\n",
    "                bboxes.append([xmin, ymin, xmax, ymax])\n",
    "            pose_est_results[i].pred_instances.areas = np.array(areas)\n",
    "            pose_est_results[i].pred_instances.bboxes = np.array(bboxes)\n",
    "\n",
    "        # track id\n",
    "        track_id, pose_est_results_last, _ = _track(data_sample,\n",
    "                                                    pose_est_results_last,\n",
    "                                                    0.3)  # Example tracking threshold\n",
    "        if track_id == -1:\n",
    "            if np.count_nonzero(keypoints[:, :, 1]) >= 3:\n",
    "                track_id = next_id\n",
    "                next_id += 1\n",
    "            else:\n",
    "                # If the number of keypoints detected is small,\n",
    "                # delete that person instance.\n",
    "                keypoints[:, :, 1] = -10\n",
    "                pose_est_results[i].pred_instances.set_field(\n",
    "                    keypoints, 'keypoints')\n",
    "                pose_est_results[i].pred_instances.set_field(\n",
    "                    pred_instances.bboxes * 0, 'bboxes')\n",
    "                pose_est_results[i].set_field(pred_instances, 'pred_instances')\n",
    "                track_id = -1\n",
    "        pose_est_results[i].set_field(track_id, 'track_id')\n",
    "\n",
    "        # convert keypoints for pose-lifting\n",
    "        pose_est_result_converted = PoseDataSample()\n",
    "        pose_est_result_converted.set_field(\n",
    "            pose_est_results[i].pred_instances.clone(), 'pred_instances')\n",
    "        pose_est_result_converted.set_field(\n",
    "            pose_est_results[i].gt_instances.clone(), 'gt_instances')\n",
    "        keypoints = convert_keypoint_definition(keypoints,\n",
    "                                                pose_det_dataset_name,\n",
    "                                                pose_lift_dataset_name)\n",
    "        \n",
    "        keypoints = [keypoints[0][:17]] #jerlin\n",
    "        #print(f\"keypoints trimmed = {[keypoints[0][:17]]}\")  \n",
    "        pose_est_result_converted.pred_instances.set_field(\n",
    "            keypoints, 'keypoints')\n",
    "        pose_est_result_converted.set_field(pose_est_results[i].track_id,\n",
    "                                            'track_id')\n",
    "        pose_est_results_converted.append(pose_est_result_converted)\n",
    "\n",
    "    pose_est_results_list.append(pose_est_results_converted.copy())\n",
    "\n",
    "    # Second stage: Pose lifting\n",
    "    # extract and pad input pose2d sequence\n",
    "    pose_seq_2d = extract_pose_sequence(\n",
    "        pose_est_results_list,\n",
    "        frame_idx=frame_idx,\n",
    "        causal=pose_lift_dataset.get('causal', False),\n",
    "        seq_len=pose_lift_dataset.get('seq_len', 1),\n",
    "        step=pose_lift_dataset.get('seq_step', 1))\n",
    "\n",
    "    # conduct 2D-to-3D pose lifting\n",
    "    pose_lift_results = inference_pose_lifter_model(\n",
    "        pose_lifter,\n",
    "        pose_seq_2d,\n",
    "        image_size=visualize_frame.shape[:2],\n",
    "        norm_pose_2d=False)  # Example setting for norm_pose_2d\n",
    "\n",
    "    # post-processing\n",
    "    for idx, pose_lift_result in enumerate(pose_lift_results):\n",
    "        pose_lift_result.track_id = pose_est_results[idx].get('track_id', 1e4)\n",
    "\n",
    "        pred_instances = pose_lift_result.pred_instances\n",
    "        keypoints = pred_instances.keypoints\n",
    "        keypoint_scores = pred_instances.keypoint_scores\n",
    "        if keypoint_scores.ndim == 3:\n",
    "            keypoint_scores = np.squeeze(keypoint_scores, axis=1)\n",
    "            pose_lift_results[\n",
    "                idx].pred_instances.keypoint_scores = keypoint_scores\n",
    "        if keypoints.ndim == 4:\n",
    "            keypoints = np.squeeze(keypoints, axis=1)\n",
    "\n",
    "        keypoints = keypoints[..., [0, 2, 1]]\n",
    "        keypoints[..., 0] = -keypoints[..., 0]\n",
    "        keypoints[..., 2] = -keypoints[..., 2]\n",
    "\n",
    "        # rebase height (z-axis)\n",
    "        keypoints[..., 2] -= np.min(\n",
    "            keypoints[..., 2], axis=-1, keepdims=True)\n",
    "\n",
    "        pose_lift_results[idx].pred_instances.keypoints = keypoints\n",
    "\n",
    "    pose_lift_results = sorted(\n",
    "        pose_lift_results, key=lambda x: x.get('track_id', 1e4))\n",
    "\n",
    "    pred_3d_data_samples = merge_data_samples(pose_lift_results)\n",
    "    det_data_sample = merge_data_samples(pose_est_results)\n",
    "    pred_3d_instances = pred_3d_data_samples.get('pred_instances', None)\n",
    "\n",
    "    # Assuming you want to visualize 1 3D pose\n",
    "    num_instances = 1 \n",
    "\n",
    "    # Visualization\n",
    "    '''if visualizer is not None:\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            visualize_frame,\n",
    "            data_sample=pred_3d_data_samples,\n",
    "            det_data_sample=det_data_sample,\n",
    "            draw_gt=False,\n",
    "            dataset_2d= pose_det_dataset_name,\n",
    "            dataset_3d=pose_lift_dataset_name,\n",
    "            show=False,\n",
    "            draw_bbox=True,\n",
    "            kpt_thr=0.3,  # Example threshold\n",
    "            num_instances=num_instances,\n",
    "            wait_time=0)'''\n",
    "\n",
    "    return pose_est_results, pose_est_results_list, pred_3d_instances, next_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca6001-d424-4622-b5a1-de55af086a97",
   "metadata": {},
   "source": [
    "#### calculate bbox overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6383c-8e02-42a8-933e-cbbbf1defbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlap(bbox_test, bbox_init):\n",
    "    '''returns overlap between test bbox and init bbox as a fraction of init bbox.\n",
    "    requires min <= max'''\n",
    "\n",
    "    bbox_test = bbox_test[0]\n",
    "    \n",
    "    # Assume `bbox_init` is a pandas DataFrame, extract values to array or list\n",
    "    bbox_init_values = bbox_init.to_numpy()\n",
    "    bbox_init = bbox_init_values[0]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap_width = min(bbox_init[1], bbox_test[2]) - max(bbox_init[0], bbox_test[0])\n",
    "    overlap_height = min(bbox_init[3], bbox_test[3]) - max(bbox_init[2], bbox_test[1])\n",
    "    if overlap_width < 0 or overlap_height < 0:\n",
    "        return 0\n",
    "    return (overlap_width * overlap_height) / ((bbox_init[1] - bbox_init[0]) * (bbox_init[3] - bbox_init[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1e7a7-b3b3-4b2b-8ac7-35445c82ad8e",
   "metadata": {},
   "source": [
    "### Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb9316-7a9b-45e8-98e4-9a2c5ad1cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Device = {device}\")\n",
    "def pose_estimate(vidfile, start, end):\n",
    "    \"\"\"Main function to estimate poses in a video.\"\"\"\n",
    "    vid_reader = get_video_reader(vidfile)\n",
    "    print(f\"Processing {vidfile}\")\n",
    "    clip_name = vidfile[-len('7942GT00.mp4'):-len('.mp4')]\n",
    "    csv_fname = \"data/pose/\" + clip_name + '_dbx.csv'\n",
    "    if os.path.exists(csv_fname):\n",
    "        print(csv_fname + ' already exists. Skipping for now')\n",
    "        return\n",
    "\n",
    "    vidfile_name = vidfile.split('/')[-1]\n",
    "    bboxes = read_bboxes()\n",
    "    init_bbox = get_init_bbox(vidfile_name, bboxes) \n",
    "    # Assume `bbox_init` is a pandas DataFrame, extract values to array or list\n",
    "    bbox_init_values = init_bbox.to_numpy()\n",
    "    bbox_init = bbox_init_values[0]\n",
    "    #convert to xyxy\n",
    "    bbox_init = [bbox_init[0], bbox_init[2], bbox_init[1], bbox_init[3]]\n",
    "    #print(f\"bbox_init={bbox_init}\")\n",
    "    \n",
    "    vid_writer = initialize_video_writer(f'data/pose/videos/{clip_name}.mp4', vid_reader.width, vid_reader.height)\n",
    "\n",
    "    #from tqdm import tqdm\n",
    "    total_frames = min(int(end * vid_reader.fps), len(vid_reader)) #tqdm\n",
    "    progress_bar = ProgressBar(task_num = total_frames)\n",
    "\n",
    "    with open(csv_fname, 'w') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        frame_num = 0\n",
    "        all_keypoints_wholebody = []\n",
    "        all_keypoints_3d = []\n",
    "        all_bboxes = []\n",
    "\n",
    "        while frame_num < total_frames:#tqdm\n",
    "            negative_frame = vid_reader[frame_num] #tqdm\n",
    "            frame_num += 1 #tqdm\n",
    "            #print(f\"frame number = {frame_num}\")\n",
    "            # Convert frame to RGB for inference\n",
    "            frame = mmcv.imread(negative_frame, channel_order='rgb')\n",
    "            progress_bar.update()\n",
    "\n",
    "            start_time = time.time()\n",
    "             # Process image and perform 2D pose estimation on whole body\n",
    "            pose_est_results = get_2d_keypoints_from_frame(detector=detector,\n",
    "                                                                      frame=frame,\n",
    "                                                                      bbox_init=bbox_init,\n",
    "                                                                      pose_estimator=pose_estimator_wholebody,\n",
    "                                                                      visualizer=visualizer)\n",
    "\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            logging.info(f\"Code block get_2d_keypoints_from_frame took {elapsed_time:.2f} seconds\")\n",
    "            #mmcv.imwrite(mmcv.rgb2bgr(processed_frame), \"data/pose/output_image.jpg\")\n",
    "\n",
    "             \n",
    "            keypoint_scores_chosen = pose_est_results.pred_instances.keypoint_scores[0]\n",
    "            keypoints_wholebody_chosen = pose_est_results.pred_instances.keypoints[0]\n",
    "            bbox_chosen = pose_est_results.pred_instances.bboxes[0]\n",
    "            data_sample_chosen = pose_est_results\n",
    "            \n",
    "            scores_expanded = np.expand_dims(keypoint_scores_chosen, axis=-1)\n",
    "            keypoints_wholebody = np.concatenate([keypoints_wholebody_chosen, scores_expanded], axis=-1)\n",
    "                \n",
    "            start_time = time.time()\n",
    "            # Process image and perform 3D pose lifting\n",
    "            pose_est_results_list = []\n",
    "            _, _, pred_3d_instances, _ = get_3d_keypoints_from_frame(detector=detector,\n",
    "                                                                      frame=frame,\n",
    "                                                                      frame_idx=0,\n",
    "                                                                      pose_estimator=pose_estimator,\n",
    "                                                                      pose_est_results_last=[],\n",
    "                                                                      pose_est_results_list=pose_est_results_list,\n",
    "                                                                      next_id=0,\n",
    "                                                                      pose_lifter=pose_lifter,\n",
    "                                                                      visualize_frame=frame,\n",
    "                                                                      visualizer=visualizer)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            logging.info(f\"Code block get_3d_keypoints_from_frame took {elapsed_time:.2f} seconds\")\n",
    "\n",
    "            keypoints_3d = pred_3d_instances.keypoints[0]\n",
    "            \n",
    "            if len(keypoints_wholebody_chosen > 0):\n",
    "                all_keypoints_wholebody.append(keypoints_wholebody)\n",
    "                all_keypoints_3d.append(keypoints_3d)\n",
    "                all_bboxes.append(bbox_chosen)\n",
    "            else:\n",
    "                all_keypoints_wholebody.append(np.zeros(133))\n",
    "                all_keypoints_3d.append(np.zeros((17, 4)))\n",
    "                all_bboxes.append(np.zeros(4))\n",
    "\n",
    "            # Visualize and save each frame to the video\n",
    "            start_time = time.time()\n",
    "            visualize_and_save_frame(frame, data_sample_chosen, vid_writer, init_bbox)\n",
    "            #plot_keypoints_opencv(frame, keypoints_wholebody_chosen, vid_writer, bbox_chosen, init_bbox)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            logging.info(f\"Code block visualize_and_save_frame took {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "        vid_writer.release()  # Release the video writer\n",
    "        \n",
    "        # Save pose results to CSV\n",
    "        save_pose_results_to_csv(csv_writer, all_keypoints_wholebody, all_keypoints_3d, all_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e4fb1-1860-44ec-b060-b3b8c4425696",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = pd.read_excel('data/EEG Video Timings.xlsx')\n",
    "type_col_name = list(filter(lambda s: s.startswith('Type'), video_data.columns))[0]\n",
    "generalized_sz = video_data[video_data[type_col_name] == 0]\n",
    "fnames = 'data/videos/' + generalized_sz['Filename']+'.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92301ccf-c168-497c-81dc-73f5e09d0c48",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490713f9-36f4-4507-b624-7eccbfa1d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname, start, end in zip(fnames, generalized_sz['Video\\nT0 to Start'], generalized_sz['Video\\nT0 to End']):\n",
    "    try:\n",
    "        start_sec = int(start.split(':')[0]) * 60 + int(start.split(':')[1])\n",
    "        end_sec = int(end.split(':')[0]) * 60 + int(end.split(':')[1])\n",
    "        pose_estimate(fname, start_sec, end_sec)\n",
    "    except ValueError as exc:\n",
    "        print('No timings available for video file: ' + fname)\n",
    "    except FileNotFoundError:\n",
    "        print('Could not find video file: ' + fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1c2be-9142-403d-b4d9-e190620ca188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7c2bb-a5a4-437c-ad03-9da6196539d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
