{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = pd.read_excel('data/EEG Video Timings.xlsx')\n",
    "type_col_name = list(filter(lambda s: s.startswith('Type'), video_data.columns))[0]\n",
    "generalized_sz = video_data[video_data[type_col_name] == 0]\n",
    "fnames = 'data/videos/' + generalized_sz['Filename']+'.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_sz['ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/df_time_find_best.pkl', 'rb') as f:\n",
    "    df_time = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time['pt_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time['vid_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_evaluate_classifier(df_time):\n",
    "    \"\"\"\n",
    "    Performs patient-level cross-validation with detailed progress tracking\n",
    "    including patient IDs and video IDs\n",
    "    \"\"\"\n",
    "    print(\"Starting classification pipeline...\")\n",
    "\n",
    "    classifiers = {}  # Dictionary to store classifier for each fold\n",
    "    \n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df_time['class'])\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in df_time.columns \n",
    "                   if col not in ['class', 'pt_id', 'vid_id']]\n",
    "    X = df_time[feature_cols]\n",
    "    \n",
    "    # Initialize GroupKFold for patient-level splitting\n",
    "    n_splits = 5\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'fold': [], 'f1': [], 'auc': [], \n",
    "        'sensitivity': [], 'specificity': [],\n",
    "        'tn': [], 'fp': [], 'fn': [], 'tp': []\n",
    "    }\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions_df = pd.DataFrame()\n",
    "    \n",
    "    # Get unique patient and video IDs\n",
    "    unique_pts = df_time['pt_id'].unique()\n",
    "    total_videos = df_time['vid_id'].nunique()\n",
    "    print(f\"\\nTotal patients: {len(unique_pts)}\")\n",
    "    print(f\"Total videos: {total_videos}\")\n",
    "    fold_splits = list(group_kfold.split(unique_pts, groups=unique_pts))\n",
    "    \n",
    "    # Random Forest parameters\n",
    "    rf_params = {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'sqrt',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'class_weight': 'balanced',\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    # Main fold loop with progress bar\n",
    "    for fold, (train_pt_idx, val_pt_idx) in enumerate(tqdm(fold_splits, desc=\"Folds\", unit=\"fold\")):\n",
    "        print(f\"\\nProcessing Fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Get patient IDs for this fold\n",
    "        train_pts = unique_pts[train_pt_idx]\n",
    "        val_pts = unique_pts[val_pt_idx]\n",
    "        \n",
    "        # Get video counts for this fold\n",
    "        train_videos = df_time[df_time['pt_id'].isin(train_pts)]['vid_id'].nunique()\n",
    "        val_videos = df_time[df_time['pt_id'].isin(val_pts)]['vid_id'].nunique()\n",
    "        \n",
    "        print(\"\\nTraining Set:\")\n",
    "        print(f\"Patients ({len(train_pts)}): {', '.join(map(str, train_pts))}\")\n",
    "        print(f\"Videos: {train_videos}\")\n",
    "        \n",
    "        print(\"\\nValidation Set:\")\n",
    "        print(f\"Patients ({len(val_pts)}): {', '.join(map(str, val_pts))}\")\n",
    "        print(f\"Videos: {val_videos}\")\n",
    "        \n",
    "        # Create train/val masks based on patient IDs\n",
    "        train_mask = df_time['pt_id'].isin(train_pts)\n",
    "        val_mask = df_time['pt_id'].isin(val_pts)\n",
    "        \n",
    "        # Split data\n",
    "        X_train = X[train_mask]\n",
    "        X_val = X[val_mask]\n",
    "        y_train = y[train_mask]\n",
    "        y_val = y[val_mask]\n",
    "        \n",
    "        print(\"\\nTraining Random Forest...\")\n",
    "        clf = RandomForestClassifier(**rf_params)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Store the classifier\n",
    "        classifiers[fold] = clf\n",
    "        \n",
    "        print(\"Making predictions...\")\n",
    "        # Process each validation patient separately\n",
    "        for pt_id in tqdm(val_pts, desc=\"Validation Patients\", unit=\"patient\"):\n",
    "            pt_mask = df_time['pt_id'] == pt_id\n",
    "            pt_videos = df_time[pt_mask]['vid_id'].unique()\n",
    "            \n",
    "            for vid_id in tqdm(pt_videos, desc=f\"Videos for Patient {pt_id}\", unit=\"video\", leave=False):\n",
    "                vid_mask = (df_time['pt_id'] == pt_id) & (df_time['vid_id'] == vid_id)\n",
    "                X_vid = X[vid_mask]\n",
    "                y_vid = y[vid_mask]\n",
    "                \n",
    "                # Make predictions for this video\n",
    "                y_pred_vid = clf.predict(X_vid)\n",
    "                y_pred_proba_vid = clf.predict_proba(X_vid)[:, 1]\n",
    "                \n",
    "                # Store predictions for this video\n",
    "                vid_predictions = pd.DataFrame({\n",
    "                    'fold': fold,\n",
    "                    'pt_id': pt_id,\n",
    "                    'vid_id': vid_id,\n",
    "                    'true_class': le.inverse_transform(y_vid),\n",
    "                    'predicted_class': le.inverse_transform(y_pred_vid),\n",
    "                    'prediction_probability': y_pred_proba_vid\n",
    "                }, index=df_time[vid_mask].index)\n",
    "                \n",
    "                predictions_df = pd.concat([predictions_df, vid_predictions])\n",
    "        \n",
    "        # Calculate metrics for the entire fold\n",
    "        y_pred = predictions_df[predictions_df['fold'] == fold]['predicted_class'].map({'nml': 0, 'sz': 1})\n",
    "        y_true = predictions_df[predictions_df['fold'] == fold]['true_class'].map({'nml': 0, 'sz': 1})\n",
    "        y_pred_proba = predictions_df[predictions_df['fold'] == fold]['prediction_probability']\n",
    "        \n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        \n",
    "        # Store metrics\n",
    "        results['fold'].append(fold)\n",
    "        results['f1'].append(f1)\n",
    "        results['auc'].append(auc)\n",
    "        results['sensitivity'].append(sensitivity)\n",
    "        results['specificity'].append(specificity)\n",
    "        results['tn'].append(tn) \n",
    "        results['fp'].append(fp) \n",
    "        results['fn'].append(fn)\n",
    "        results['tp'].append(tp)\n",
    "        \n",
    "        # Print current fold metrics\n",
    "        print(f\"\\nFold {fold + 1} Metrics:\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"[[{tn}, {fp}],\")\n",
    "        print(f\" [{fn}, {tp}]]\")\n",
    "        print(f\"F1 Score: {f1:.3f}\")\n",
    "        print(f\"AUC: {auc:.3f}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "        print(f\"Specificity: {specificity:.3f}\")\n",
    "        print(\"*\" * 80)\n",
    "    \n",
    "    # Sort by original index\n",
    "    predictions_df = predictions_df.sort_index()\n",
    "    \n",
    "    # Calculate and print average metrics\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    print(f\"\\n{BLUE}Final Average Metrics:{RESET}\")\n",
    "    print(f\"{BLUE}F1 Score: {metrics_df['f1'].mean():.3f} ± {metrics_df['f1'].std():.3f}{RESET}\")\n",
    "    print(f\"{BLUE}AUC: {metrics_df['auc'].mean():.3f} ± {metrics_df['auc'].std():.3f}{RESET}\")\n",
    "    print(f\"{BLUE}Sensitivity: {metrics_df['sensitivity'].mean():.3f} ± {metrics_df['sensitivity'].std():.3f}{RESET}\")\n",
    "    print(f\"{BLUE}Specificity: {metrics_df['specificity'].mean():.3f} ± {metrics_df['specificity'].std():.3f}{RESET}\")\n",
    "    \n",
    "    return metrics_df, predictions_df, classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df, predictions_df, classifiers = train_evaluate_classifier(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"data/fold_metrics_RF_150_300_without_detection.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_pickle(\"data/predictions_150_300_without_detection.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Level AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_video_auc(df):\n",
    "    \"\"\"\n",
    "    Calculate AUC for each unique video ID in the dataset.\n",
    "    Returns a DataFrame with video-level AUC scores.\n",
    "    \"\"\"\n",
    "    # Initialize storage for results\n",
    "    video_metrics = []\n",
    "    \n",
    "    # Get unique video IDs\n",
    "    unique_videos = df[['pt_id', 'vid_id']].drop_duplicates()\n",
    "    \n",
    "    for _, row in unique_videos.iterrows():\n",
    "        pt_id = row['pt_id']\n",
    "        vid_id = row['vid_id']\n",
    "        \n",
    "        # Get data for this video\n",
    "        video_data = df[(df['pt_id'] == pt_id) & (df['vid_id'] == vid_id)]\n",
    "        \n",
    "        # Convert classes to binary\n",
    "        y_true = (video_data['true_class'] == 'sz').astype(int)\n",
    "        \n",
    "        # Get prediction probabilities\n",
    "        y_pred = video_data['prediction_probability']\n",
    "        \n",
    "        try:\n",
    "            # Calculate AUC if we have both classes\n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                auc = roc_auc_score(y_true, y_pred)\n",
    "            else:\n",
    "                auc = np.nan  # Can't calculate AUC with only one class\n",
    "                \n",
    "            video_metrics.append({\n",
    "                'pt_id': pt_id,\n",
    "                'vid_id': vid_id,\n",
    "                'auc': auc,\n",
    "                'n_samples': len(video_data),\n",
    "                'n_seizure': sum(y_true),\n",
    "                'n_normal': sum(y_true == 0),\n",
    "                'calculable': len(np.unique(y_true)) > 1\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating AUC for video {vid_id} (patient {pt_id}): {str(e)}\")\n",
    "            \n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame(video_metrics)\n",
    "    \n",
    "    # Add some summary statistics\n",
    "    print(\"\\nSummary of video-level AUC scores:\")\n",
    "    print(f\"Total videos: {len(results_df)}\")\n",
    "    print(f\"Videos with calculable AUC: {sum(results_df['calculable'])}\")\n",
    "    calculable_aucs = results_df[results_df['calculable']]['auc']\n",
    "    if len(calculable_aucs) > 0:\n",
    "        print(f\"Mean AUC: {calculable_aucs.mean():.3f} ± {calculable_aucs.std():.3f}\")\n",
    "        print(f\"Median AUC: {calculable_aucs.median():.3f}\")\n",
    "        print(f\"Min AUC: {calculable_aucs.min():.3f}\")\n",
    "        print(f\"Max AUC: {calculable_aucs.max():.3f}\")\n",
    "    \n",
    "    return results_df\n",
    "video_auc = calculate_video_auc(predictions_df)\n",
    "video_auc.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_auc.sort_values('auc', ascending=False).to_csv(\"data/auc_videos_RF_150_300_without_detection.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patient level AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_patient_auc(df):\n",
    "    \"\"\"\n",
    "    Calculate AUC for each unique patient ID in the dataset.\n",
    "    Returns a DataFrame with patient-level AUC scores.\n",
    "    \"\"\"\n",
    "    # Initialize storage for results\n",
    "    patient_metrics = []\n",
    "    \n",
    "    # Get unique patient IDs\n",
    "    unique_patients = df['pt_id'].unique()\n",
    "    \n",
    "    for pt_id in unique_patients:\n",
    "        # Get all data for this patient\n",
    "        patient_data = df[df['pt_id'] == pt_id]\n",
    "        \n",
    "        # Convert classes to binary\n",
    "        y_true = (patient_data['true_class'] == 'sz').astype(int)\n",
    "        \n",
    "        # Get prediction probabilities\n",
    "        y_pred = patient_data['prediction_probability']\n",
    "        \n",
    "        try:\n",
    "            # Calculate AUC if we have both classes\n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                auc = roc_auc_score(y_true, y_pred)\n",
    "            else:\n",
    "                auc = np.nan  # Can't calculate AUC with only one class\n",
    "                \n",
    "            patient_metrics.append({\n",
    "                'pt_id': pt_id,\n",
    "                'auc': auc,\n",
    "                'n_samples': len(patient_data),\n",
    "                'n_videos': patient_data['vid_id'].nunique(),\n",
    "                'n_seizure': sum(y_true),\n",
    "                'n_normal': sum(y_true == 0),\n",
    "                'calculable': len(np.unique(y_true)) > 1\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating AUC for patient {pt_id}: {str(e)}\")\n",
    "            \n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame(patient_metrics)\n",
    "    \n",
    "    # Add some summary statistics\n",
    "    print(\"\\nSummary of patient-level AUC scores:\")\n",
    "    print(f\"Total patients: {len(results_df)}\")\n",
    "    print(f\"Patients with calculable AUC: {sum(results_df['calculable'])}\")\n",
    "    calculable_aucs = results_df[results_df['calculable']]['auc']\n",
    "    if len(calculable_aucs) > 0:\n",
    "        print(f\"Mean AUC: {calculable_aucs.mean():.3f} ± {calculable_aucs.std():.3f}\")\n",
    "        print(f\"Median AUC: {calculable_aucs.median():.3f}\")\n",
    "        print(f\"Min AUC: {calculable_aucs.min():.3f}\")\n",
    "        print(f\"Max AUC: {calculable_aucs.max():.3f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Use it like this:\n",
    "patient_auc = calculate_patient_auc(predictions_df)\n",
    "patient_auc.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_auc.sort_values('auc', ascending=False).to_csv(\"data/auc_patients_RF_150_300_without_detection.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_predictions(predictions_df, fold=None, patient_id=None, figsize=(15, 6)):\n",
    "    \"\"\"\n",
    "    Plot predictions and probabilities for videos, filtered by fold and/or patient_id.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions_df: DataFrame with predictions\n",
    "    fold: specific fold to plot (optional)\n",
    "    patient_id: specific patient to plot (optional)\n",
    "    figsize: tuple for figure size\n",
    "    \"\"\"\n",
    "    # Filter data if fold or patient_id specified\n",
    "    plot_df = predictions_df.copy()\n",
    "    if fold is not None:\n",
    "        plot_df = plot_df[plot_df['fold'] == fold]\n",
    "    if patient_id is not None:\n",
    "        plot_df = plot_df[plot_df['pt_id'] == patient_id]\n",
    "    \n",
    "    # Get unique combinations of fold, patient, and video\n",
    "    combinations = plot_df.groupby(['fold', 'pt_id', 'vid_id']).size().reset_index()\n",
    "    \n",
    "    # Plot each video\n",
    "    for _, row in combinations.iterrows():\n",
    "        fold_num, pt_id, vid_id = row['fold'], row['pt_id'], row['vid_id']\n",
    "        \n",
    "        # Filter data for this video\n",
    "        video_data = plot_df[\n",
    "            (plot_df['fold'] == fold_num) & \n",
    "            (plot_df['pt_id'] == pt_id) & \n",
    "            (plot_df['vid_id'] == vid_id)\n",
    "        ].copy()\n",
    "        \n",
    "        # Reset index to get frame numbers if not already present\n",
    "        if 'frame' not in video_data.columns:\n",
    "            video_data = video_data.reset_index()\n",
    "            video_data = video_data.rename(columns={'index': 'frame'})\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot probability\n",
    "        sns.lineplot(data=video_data, x='frame', y='prediction_probability', \n",
    "                    color='blue', label='Probability')\n",
    "        \n",
    "        # Plot binary predictions (convert to 0/1)\n",
    "        binary_preds = (video_data['predicted_class'] == 'sz').astype(int)\n",
    "        sns.lineplot(data=video_data, x='frame', y=binary_preds, \n",
    "                    color='green', drawstyle='steps-post', label='Prediction (0=nml, 1=sz)')\n",
    "        \n",
    "        # Add reference line at 0.5\n",
    "        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title(f'Fold {fold_num}, Patient {pt_id}, Video {vid_id}')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Probability / Prediction')\n",
    "        plt.ylim(-0.1, 1.1)  # Add some padding to y-axis\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Show plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_all_predictions(predictions_df):\n",
    "    \"\"\"\n",
    "    Plot all predictions organized by fold and patient\n",
    "    \"\"\"\n",
    "    # Get unique folds\n",
    "    folds = sorted(predictions_df['fold'].unique())\n",
    "    \n",
    "    print(\"Starting visualization...\")\n",
    "    \n",
    "    # Plot for each fold\n",
    "    for fold in folds:\n",
    "        print(f\"\\nProcessing Fold {fold}\")\n",
    "        \n",
    "        # Get unique patients in this fold\n",
    "        patients = sorted(predictions_df[predictions_df['fold'] == fold]['pt_id'].unique())\n",
    "        \n",
    "        for pt_id in patients:\n",
    "            print(f\"Processing Patient {pt_id}\")\n",
    "            plot_predictions(predictions_df, fold=fold, patient_id=pt_id)\n",
    "            \n",
    "    print(\"\\nVisualization complete!\")\n",
    "\n",
    "# Example usage:\n",
    "# For specific fold and patient:\n",
    "#plot_predictions(predictions_df, fold=0, patient_id=1)\n",
    "\n",
    "# For all predictions:\n",
    "# plot_all_predictions(predictions_df)\n",
    "\n",
    "# For all videos of a specific patient across folds:\n",
    "# plot_predictions(predictions_df, patient_id=1)\n",
    "\n",
    "# For all videos in a specific fold:\n",
    "plot_predictions(predictions_df, fold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_feature_importance(clf, feature_names, top_n=10, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Plot top N feature importances from the Random Forest classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    clf: Trained RandomForestClassifier\n",
    "    feature_names: List of feature names\n",
    "    top_n: Number of top features to show (default=10)\n",
    "    figsize: Figure size tuple (default=(10,6))\n",
    "    \"\"\"\n",
    "    # Get feature importances and names\n",
    "    importances = clf.feature_importances_\n",
    "    \n",
    "    # Create DataFrame with feature names and importances\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance and get top N\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).head(top_n)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot horizontal bar chart\n",
    "    sns.barplot(data=importance_df, \n",
    "                y='Feature', \n",
    "                x='Importance',\n",
    "                palette='viridis')\n",
    "    \n",
    "    plt.title(f'Top {top_n} Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the importance DataFrame\n",
    "    return importance_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have feature names and trained classifier:\n",
    "#feature_names = [col for col in df_time.columns if col not in ['class', 'pt_id', 'vid_id']]\n",
    "#importance_df = plot_feature_importance(clf, feature_names)\n",
    "# \n",
    "# Print exact values:\n",
    "# print(\"\\nFeature Importance Values:\")\n",
    "# print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Plot feature importance for each fold\n",
    "for fold, clf in classifiers.items():\n",
    "    print(f\"\\nFeature Importances for Fold {fold}\")\n",
    "    importance_df = plot_feature_importance(clf, feature_names)\n",
    "    print(\"\\nTop 10 Features and their Importance Values:\")\n",
    "    print(importance_df.to_string(index=False))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature importance across folds\n",
    "feature_names = [col for col in df_time.columns if col not in ['class', 'pt_id', 'vid_id']]\n",
    "avg_importances = np.zeros(len(feature_names))\n",
    "for clf in classifiers.values():\n",
    "    avg_importances += clf.feature_importances_\n",
    "\n",
    "avg_importances /= len(classifiers)\n",
    "\n",
    "# Create temporary classifier object to hold average importances\n",
    "temp_clf = type('obj', (), {'feature_importances_': avg_importances})\n",
    "\n",
    "print(\"\\nAverage Feature Importances Across All Folds:\")\n",
    "importance_df = plot_feature_importance(temp_clf, feature_names)\n",
    "print(\"\\nTop 10 Features and their Average Importance Values:\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process videos with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "def overlay_predictions_on_video(video_path, predictions, probabilities, start_frame, output_path, max_frame=None):\n",
    "    \"\"\"\n",
    "    Overlay progressive seizure prediction probabilities graph onto the video.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to input video file\n",
    "        predictions (np.array): Predicted classes for each frame\n",
    "        probabilities (np.array): Prediction probabilities for each frame\n",
    "        start_frame (int): Frame number where seizure starts\n",
    "        output_path (str): Path to save output video\n",
    "        max_frame (int, optional): Maximum number of frames to process. If None, process all frames.\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    print(f\"Overlaying predictions on video {video_path}\")\n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Limit frames if max_frame is set\n",
    "    if max_frame is not None:\n",
    "        total_frames = min(total_frames, max_frame)\n",
    "    \n",
    "    # Create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Prepare probability plot\n",
    "    plt.figure(figsize=(width / 100, height / 2.5 / 100), dpi=100)  # Make the graph taller\n",
    "    plt.style.use('dark_background')  # Dark background for better contrast\n",
    "    \n",
    "    # Process each frame\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and (max_frame is None or frame_count < max_frame):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Clear previous plot\n",
    "        plt.clf()\n",
    "        \n",
    "        # Plot probabilities up to current frame\n",
    "        probs = probabilities[:, 1] if len(probabilities.shape) > 1 else probabilities\n",
    "        \n",
    "        # Determine the range of frames to plot\n",
    "        plot_start = 0\n",
    "        #plot_end = frame_count + 1\n",
    "        plot_end = min(frame_count + 1, len(probs))\n",
    "\n",
    "        \n",
    "        # Plot probabilities\n",
    "        plt.plot(range(plot_start, plot_end), probs[plot_start:plot_end], color='black', linewidth=2, label='Probability')\n",
    "        \n",
    "        # Styling the graph\n",
    "        plt.xlim(plot_start, total_frames)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.axhline(y=0.5, color='orange', linestyle='--', linewidth=2, label='Threshold (0.5)')  # Add threshold line\n",
    "        plt.title(f'Seizure Probability (Frame {frame_count})')\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Probability')\n",
    "        \n",
    "        # Highlight seizure start frame\n",
    "        plt.axvline(x=start_frame, color='green', linestyle='--', linewidth=2, label='Seizure Start')\n",
    "        #plt.legend(loc='upper right')\n",
    "        \n",
    "        # Make plot background transparent\n",
    "        plt.gcf().patch.set_alpha(0)\n",
    "        plt.gca().patch.set_alpha(0)\n",
    "        \n",
    "        # Render graph to image\n",
    "        plt.tight_layout()\n",
    "        canvas = FigureCanvas(plt.gcf())\n",
    "        canvas.draw()\n",
    "        \n",
    "        # Convert plot to image\n",
    "        graph_image = np.frombuffer(canvas.buffer_rgba(), dtype='uint8')\n",
    "        #graph_image = graph_image.reshape(canvas.get_width_height()[::-1] + (3,))\n",
    "        graph_image = graph_image.reshape(canvas.get_width_height()[::-1] + (4,))  # RGBA\n",
    "\n",
    "        # Remove the alpha channel to use RGB\n",
    "        graph_image = graph_image[:, :, :3]\n",
    "        \n",
    "        # Resize graph to fit video width\n",
    "        graph_image = cv2.resize(graph_image, (width, int(height / 2.5)))\n",
    "        \n",
    "        # Create a semi-transparent overlay\n",
    "        overlay = frame.copy()\n",
    "        overlay[height - graph_image.shape[0]:height, :] = graph_image\n",
    "        \n",
    "        # Blend the overlay\n",
    "        frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
    "        \n",
    "        # Add probability text\n",
    "        '''if frame_count < len(probs):\n",
    "            prob = probs[frame_count]\n",
    "            text = f\"Seizure Prob: {prob:.2f}\"\n",
    "            cv2.putText(frame, text, (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            # Highlight seizure start frame\n",
    "            if frame_count == start_frame:\n",
    "                cv2.rectangle(frame, (0, 0), (width, height), (0, 255, 0), 10)\n",
    "                cv2.putText(frame, \"SEIZURE STARTS\", (width // 2 - 200, height - 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)'''\n",
    "        \n",
    "        # Write the frame\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release resources\n",
    "    plt.close()\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.groupby([\"fold\", \"pt_id\", \"vid_id\"]).size().reset_index()[[\"fold\", \"pt_id\", \"vid_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "patient_videos = generalized_sz[['ID','Filename', 'Video\\nT0 to Start', 'Video\\nT0 to End']]\n",
    "for _, row in patient_videos.iterrows():\n",
    "    video_filename =  row['Filename'] + \".mp4\"\n",
    "    video_path = os.path.join(\"data/pose/videos\", video_filename)\n",
    "    output_path = os.path.join(\"data/prediction_videos/150_300_without_detection\", f'{row[\"ID\"]}_{video_filename}')\n",
    "    #start_time = row['Video\\nT0 to Start']\n",
    "\n",
    "    # Parse start time\n",
    "    try:\n",
    "        minutes, seconds = row['Video\\nT0 to Start'].split(':')\n",
    "        start_frame = (int(minutes) * 60 + int(seconds)) * 30\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # Filter rows for the given pt_id and vid_id\n",
    "    filtered_rows = predictions_df[(predictions_df[\"pt_id\"] == row['ID']) & (predictions_df[\"vid_id\"] == row['Filename'])]\n",
    "    predictions = filtered_rows[\"predicted_class\"].map({\"nml\": 0, \"sz\": 1})\n",
    "    probabilities = filtered_rows[\"prediction_probability\"]\n",
    "    if len(probabilities) == 0:\n",
    "        print(f\"No data found for video {row['Filename']}\")\n",
    "        continue\n",
    "    overlay_predictions_on_video(\n",
    "            video_path, \n",
    "            predictions, \n",
    "            probabilities,\n",
    "            start_frame,\n",
    "            output_path,\n",
    "            max_frame=None\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process videos using a marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "def overlay_predictions_marker_on_video(video_path, predictions, probabilities, start_frame, output_path, max_frame=None):\n",
    "    \"\"\"\n",
    "    Overlay full seizure prediction graph with a moving marker onto the video.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to input video file\n",
    "        predictions (np.array): Predicted classes for each frame\n",
    "        probabilities (np.array): Prediction probabilities for each frame\n",
    "        start_frame (int): Frame number where seizure starts\n",
    "        output_path (str): Path to save output video\n",
    "        max_frame (int, optional): Maximum number of frames to process. If None, process all frames.\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    print(f\"Overlaying predictions on video {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Limit frames if max_frame is set\n",
    "    if max_frame is not None:\n",
    "        total_frames = min(total_frames, max_frame)\n",
    "    \n",
    "    # Create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = probabilities[:, 1] if len(probabilities.shape) > 1 else probabilities\n",
    "    \n",
    "    # Process each frame\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and (max_frame is None or frame_count < max_frame):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Create new figure for each frame\n",
    "        plt.figure(figsize=(width / 100, height / 2.5 / 100), dpi=100)\n",
    "        plt.style.use('dark_background')\n",
    "        \n",
    "        # Plot full probability line\n",
    "        plt.plot(range(len(probs)), probs, color='black', linewidth=2, label='Probability')\n",
    "        \n",
    "        # Add current position marker (vertical line)\n",
    "        plt.axvline(x=frame_count, color='red', linewidth=2, label='Current Frame')\n",
    "        \n",
    "        # Add current probability point\n",
    "        if frame_count < len(probs):\n",
    "            plt.plot(frame_count, probs[frame_count], 'ro', markersize=10)\n",
    "        \n",
    "        # Styling the graph\n",
    "        plt.xlim(0, total_frames)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.axhline(y=0.5, color='orange', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "        plt.axvline(x=start_frame, color='green', linestyle='--', linewidth=2, label='Seizure Start')\n",
    "        plt.title(f'Seizure Probability (Frame {frame_count})')\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Probability')\n",
    "        \n",
    "        # Make plot background transparent\n",
    "        plt.gcf().patch.set_alpha(0)\n",
    "        plt.gca().patch.set_alpha(0)\n",
    "        \n",
    "        # Render graph to image\n",
    "        plt.tight_layout()\n",
    "        canvas = FigureCanvas(plt.gcf())\n",
    "        canvas.draw()\n",
    "        \n",
    "        # Convert plot to image\n",
    "        graph_image = np.frombuffer(canvas.buffer_rgba(), dtype='uint8')\n",
    "        graph_image = graph_image.reshape(canvas.get_width_height()[::-1] + (4,))\n",
    "        graph_image = graph_image[:, :, :3]\n",
    "        \n",
    "        # Resize graph to fit video width\n",
    "        graph_image = cv2.resize(graph_image, (width, int(height / 2.5)))\n",
    "        \n",
    "        # Create a semi-transparent overlay\n",
    "        overlay = frame.copy()\n",
    "        overlay[height - graph_image.shape[0]:height, :] = graph_image\n",
    "        \n",
    "        # Blend the overlay\n",
    "        frame = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
    "        \n",
    "        # Write the frame\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "        \n",
    "        # Clear the current figure\n",
    "        plt.close()\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "patient_videos = generalized_sz[['ID','Filename', 'Video\\nT0 to Start', 'Video\\nT0 to End']]\n",
    "for _, row in patient_videos.iterrows():\n",
    "    video_filename =  row['Filename'] + \".mp4\"\n",
    "    video_path = os.path.join(\"data/pose/videos\", video_filename)\n",
    "    output_path = os.path.join(\"data/prediction_videos/150_300\", f'{row[\"ID\"]}_{video_filename}')\n",
    "    #start_time = row['Video\\nT0 to Start']\n",
    "\n",
    "    # Parse start time\n",
    "    try:\n",
    "        minutes, seconds = row['Video\\nT0 to Start'].split(':')\n",
    "        start_frame = (int(minutes) * 60 + int(seconds)) * 30\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    # Filter rows for the given pt_id and vid_id\n",
    "    filtered_rows = predictions_df[(predictions_df[\"pt_id\"] == row['ID']) & (predictions_df[\"vid_id\"] == row['Filename'])]\n",
    "    predictions = filtered_rows[\"predicted_class\"].map({\"nml\": 0, \"sz\": 1})\n",
    "    probabilities = filtered_rows[\"prediction_probability\"]\n",
    "    if len(probabilities) == 0:\n",
    "        print(f\"No data found for video {row['Filename']}\")\n",
    "        continue\n",
    "    overlay_predictions_marker_on_video(\n",
    "            video_path, \n",
    "            predictions, \n",
    "            probabilities,\n",
    "            start_frame,\n",
    "            output_path,\n",
    "            max_frame=None\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### F1 score per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def calculate_patient_f1(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate F1 score and related metrics for each unique patient ID in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with columns ['pt_id', 'true_class', 'prediction_probability']\n",
    "    threshold: float, threshold for converting probabilities to binary predictions\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with patient-level metrics\n",
    "    \"\"\"\n",
    "    # Initialize storage for results\n",
    "    patient_metrics = []\n",
    "    \n",
    "    # Get unique patient IDs\n",
    "    unique_patients = df['pt_id'].unique()\n",
    "    \n",
    "    for pt_id in unique_patients:\n",
    "        # Get data for this patient\n",
    "        patient_data = df[df['pt_id'] == pt_id]\n",
    "        \n",
    "        # Convert classes to binary\n",
    "        y_true = (patient_data['true_class'] == 'sz').astype(int)\n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        y_pred = (patient_data['prediction_probability'] >= threshold).astype(int)\n",
    "        \n",
    "        try:\n",
    "            # Calculate metrics if we have both classes\n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                f1 = f1_score(y_true, y_pred)\n",
    "                precision = precision_score(y_true, y_pred)\n",
    "                recall = recall_score(y_true, y_pred)\n",
    "                calculable = True\n",
    "            else:\n",
    "                f1 = precision = recall = np.nan\n",
    "                calculable = False\n",
    "                \n",
    "            patient_metrics.append({\n",
    "                'pt_id': pt_id,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'n_samples': len(patient_data),\n",
    "                'n_seizure': sum(y_true),\n",
    "                'n_normal': sum(y_true == 0),\n",
    "                'seizure_ratio': sum(y_true) / len(y_true),\n",
    "                 'calculable': calculable\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics for patient {pt_id}: {str(e)}\")\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame(patient_metrics)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    print(\"\\nSummary of patient-level metrics:\")\n",
    "    print(f\"Total patients: {len(results_df)}\")\n",
    "    print(f\"Patients with calculable metrics: {sum(results_df['calculable'])}\")\n",
    "    \n",
    "    calculable_metrics = results_df[results_df['calculable']]\n",
    "    if len(calculable_metrics) > 0:\n",
    "        print(f\"\\nF1 Score:\")\n",
    "        print(f\"Mean: {calculable_metrics['f1'].mean():.3f} ± {calculable_metrics['f1'].std():.3f}\")\n",
    "        print(f\"Median: {calculable_metrics['f1'].median():.3f}\")\n",
    "        print(f\"\\nPrecision:\")\n",
    "        print(f\"Mean: {calculable_metrics['precision'].mean():.3f} ± {calculable_metrics['precision'].std():.3f}\")\n",
    "        print(f\"\\nRecall:\")\n",
    "        print(f\"Mean: {calculable_metrics['recall'].mean():.3f} ± {calculable_metrics['recall'].std():.3f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "patient_metrics = calculate_patient_f1(predictions_df)\n",
    "patient_metrics.sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def calculate_video_f1(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate F1 score and related metrics for each unique video ID in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with columns ['pt_id', 'vid_id', 'true_class', 'prediction_probability']\n",
    "    threshold: float, threshold for converting probabilities to binary predictions\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with video-level metrics\n",
    "    \"\"\"\n",
    "    # Initialize storage for results\n",
    "    video_metrics = []\n",
    "    \n",
    "    # Get unique video IDs\n",
    "    unique_videos = df[['pt_id', 'vid_id']].drop_duplicates()\n",
    "    \n",
    "    for _, row in unique_videos.iterrows():\n",
    "        pt_id = row['pt_id']\n",
    "        vid_id = row['vid_id']\n",
    "        \n",
    "        # Get data for this video\n",
    "        video_data = df[(df['pt_id'] == pt_id) & (df['vid_id'] == vid_id)]\n",
    "        \n",
    "        # Convert classes to binary\n",
    "        y_true = (video_data['true_class'] == 'sz').astype(int)\n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        y_pred = (video_data['prediction_probability'] >= threshold).astype(int)\n",
    "        \n",
    "        try:\n",
    "            # Calculate metrics if we have both classes\n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                f1 = f1_score(y_true, y_pred)\n",
    "                precision = precision_score(y_true, y_pred)\n",
    "                recall = recall_score(y_true, y_pred)\n",
    "                calculable = True\n",
    "            else:\n",
    "                f1 = precision = recall = np.nan\n",
    "                calculable = False\n",
    "                \n",
    "            video_metrics.append({\n",
    "                'pt_id': pt_id,\n",
    "                'vid_id': vid_id,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'n_samples': len(video_data),\n",
    "                'n_seizure': sum(y_true),\n",
    "                'n_normal': sum(y_true == 0),\n",
    "                'seizure_ratio': sum(y_true) / len(y_true),\n",
    "                'calculable': calculable\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics for video {vid_id} (patient {pt_id}): {str(e)}\")\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame(video_metrics)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    print(\"\\nSummary of video-level metrics:\")\n",
    "    print(f\"Total videos: {len(results_df)}\")\n",
    "    print(f\"Videos with calculable metrics: {sum(results_df['calculable'])}\")\n",
    "    \n",
    "    calculable_metrics = results_df[results_df['calculable']]\n",
    "    if len(calculable_metrics) > 0:\n",
    "        print(f\"\\nF1 Score:\")\n",
    "        print(f\"Mean: {calculable_metrics['f1'].mean():.3f} ± {calculable_metrics['f1'].std():.3f}\")\n",
    "        print(f\"Median: {calculable_metrics['f1'].median():.3f}\")\n",
    "        print(f\"\\nPrecision:\")\n",
    "        print(f\"Mean: {calculable_metrics['precision'].mean():.3f} ± {calculable_metrics['precision'].std():.3f}\")\n",
    "        print(f\"\\nRecall:\")\n",
    "        print(f\"Mean: {calculable_metrics['recall'].mean():.3f} ± {calculable_metrics['recall'].std():.3f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "video_f1 = calculate_video_f1(predictions_df)\n",
    "video_f1.sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cceb1fb107b2bbf4aedbf2536566f5eb68dedbd1bb981e112f990f33e36b1b78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
